{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65fa2b56",
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install huggingface_hub==0.36.1 transformers==4.57.6 transformer_lens==2.17.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f958edf3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import huggingface_hub\n",
        "assert huggingface_hub.__version__ == \"0.36.1\", \\\n",
        "    f\"RESTART KERNEL! Got {huggingface_hub.__version__}, need 0.36.1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Imports\n",
        "import torch\n",
        "from huggingface_hub import list_repo_refs\n",
        "from transformer_lens import HookedTransformer\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Check GPU\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
        "print(\n",
        "    f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"N/A\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Verify checkpoint availability\n",
        "MODELS = [\n",
        "    \"EleutherAI/pythia-160m-deduped\",\n",
        "    \"EleutherAI/pythia-1b-deduped\",\n",
        "    \"EleutherAI/pythia-2.8b-deduped\",\n",
        "]\n",
        "CHECKPOINT_INDICES = [0, 15, 30, 60, 90, 120, 140, 150, 152, 153]\n",
        "\n",
        "for model in MODELS:\n",
        "    print(f\"\\n=== {model} ===\")\n",
        "    refs = list_repo_refs(model)\n",
        "    available = [r.name for r in refs.branches if \"step\" in r.name]\n",
        "    required = [f\"step{i * 1000}\" for i in CHECKPOINT_INDICES]\n",
        "    missing = set(required) - set(available)\n",
        "    print(f\"Available: {len(available)} step revisions\")\n",
        "    print(f\"Missing required: {missing if missing else 'None'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Test model loading (smallest first)\n",
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-160m\")\n",
        "print(\"model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Test checkpoint loading (corrected — uses revision parameter)\n",
        "try:\n",
        "    from transformers import AutoModelForCausalLM as _AMCLM\n",
        "\n",
        "    hf_model = _AMCLM.from_pretrained(\n",
        "        \"EleutherAI/pythia-160m-deduped\",\n",
        "        revision=\"step30000\",\n",
        "        torch_dtype=torch.float32,\n",
        "    )\n",
        "    model_step = HookedTransformer.from_pretrained(\n",
        "        \"EleutherAI/pythia-160m-deduped\",\n",
        "        hf_model=hf_model,\n",
        "        tokenizer=AutoTokenizer.from_pretrained(\n",
        "            \"EleutherAI/pythia-160m-deduped\", revision=\"step30000\"\n",
        "        ),\n",
        "    )\n",
        "    print(\"✅ Checkpoint loading works (via HF revision parameter)!\")\n",
        "    del hf_model, model_step\n",
        "except Exception as e:\n",
        "    print(f\"❌ Checkpoint loading failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Memory test with 2.8B\n",
        "print(\"\\nLoading pythia-2.8b (this is the critical test)...\")\n",
        "start = time.time()\n",
        "model_2_8b = HookedTransformer.from_pretrained(\"pythia-2.8b-deduped\")\n",
        "load_time = time.time() - start\n",
        "print(f\"Loaded in {load_time:.1f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Attention extraction speed test\n",
        "test_input = \"A screen reader is\"\n",
        "tokens = model_2_8b.to_tokens(test_input)\n",
        "\n",
        "start = time.time()\n",
        "logits, cache = model_2_8b.run_with_cache(tokens)\n",
        "cache_time = time.time() - start\n",
        "print(f\"Cache extraction: {cache_time:.2f}s\")\n",
        "print(\n",
        "    f\"Cache memory: {sum(v.element_size() * v.nelement() for v in cache.values()) / 1e6:.1f} MB\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
