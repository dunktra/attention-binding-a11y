@article{olah2020zoom,
  title = {Zoom In: An Introduction to Circuits},
  author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  journal = {Distill},
  year = {2020},
  doi = {10.23915/distill.00024.001}
}

@article{elhage2021transformer,
  title={A Mathematical Framework for Transformer Circuits},
  author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and others},
  journal={Transformer Circuits Thread},
  year={2021}
}

@article{wei2022emergent,
  title={Emergent Abilities of Large Language Models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@article{schaeffer2023emergence,
  title={Are Emergent Abilities of Large Language Models a Mirage?},
  author={Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  journal={arXiv preprint arXiv:2304.15004},
  year={2023}
}

@article{kaplan2020scaling,
  title={Scaling Laws for Neural Language Models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{biderman2023pythia,
  title={Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin and others},
  journal={arXiv preprint arXiv:2304.01373},
  year={2023}
}

@article{bricken2023monosemanticity,
  title={Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
  author={Bricken, Trenton and Templeton, Adly and Batson, Joshua and others},
  journal={Transformer Circuits Thread},
  year={2023}
}

@article{burns2022discovering,
  title={Discovering Latent Knowledge in Language Models Without Supervision},
  author={Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2212.03827},
  year={2022}
}

@article{olsson2022context,
  title={In-context Learning and Induction Heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and others},
  journal={Transformer Circuits Thread},
  year={2022}
}

@article{jain2019attention,
  title={Attention is Not Explanation},
  author={Jain, Sarthak and Wallace, Byron C.},
  journal={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics},
  pages={3543--3556},
  year={2019}
}

@article{wiegreffe2019attention,
  title={Attention is Not Not Explanation},
  author={Wiegreffe, Sarah and Pinter, Yuval},
  journal={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing},
  year={2019}
}



@inproceedings{clark2019what,
  title={What Does {BERT} Look At? An Analysis of {BERT}'s Attention},
  author={Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
  booktitle={Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={276--286},
  year={2019},
  publisher={Association for Computational Linguistics},
  address={Florence, Italy}
}

@article{voita2019analyzing,
  title={Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned},
  author={Voita, Elena and Talbot, David and Moiseev, Fedor and others},
  journal={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={5797--5808},
  year={2019}
}

@inproceedings{michel2019sixteen,
  title={Are Sixteen Heads Really Better than One?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}

@article{wang2023interpretability,
  title={Interpretability in the Wild: A Circuit for Indirect Object Identification in GPT-2 Small},
  author={Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and others},
  journal={Proceedings of the Eleventh International Conference on Learning Representations (ICLR)},
  year={2023}
}

@article{goldowsky2023localizing,
  title={Localizing Model Behavior with Path Patching},
  author={Goldowsky-Dill, N. and MacLeod, Chris and Sato, Lucas and Arora, Aryaman},
  journal={arXiv preprint arXiv:2304.05969},
  year={2023}
}

@misc{nanda2023transformerlens,
  title={TransformerLens: A Library for Mechanistic Interpretability of Generative Language Models},
  author={Nanda, Neel and Meyer, Bryce},
  year={2023},
  url={https://github.com/TransformerLensOrg/TransformerLens}
}

@article{gao2020pile,
  title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@article{swayamdipta2020dataset,
  title={Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics},
  author={Swayamdipta, Swabha and Schwartz, Roy and Lourie, Nicholas and others},
  journal={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  pages={9275--9293},
  year={2020}
}

@article{cunningham2023sparse,
  title={Sparse Autoencoders Find Highly Interpretable Features in Language Models},
  author={Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and others},
  journal={arXiv preprint arXiv:2309.08600},
  year={2023}
}

@misc{w3c2018wcag,
  title={Web Content Accessibility Guidelines (WCAG) 2.1},
  author={{W3C}},
  year={2018},
  howpublished={W3C Recommendation},
  url={https://www.w3.org/WAI/WCAG21/Understanding/}
}

@article{trewin2019considerations,
  title={Considerations for AI Fairness for People with Disabilities},
  author={Trewin, Shari and others},
  journal={AI Matters},
  volume={5},
  number={3},
  pages={40--63},
  year={2019}
}

@misc{salas2026testing,
  title={Testing Accessibility Knowledge Across Pythia Model Sizes},
  author={Salas, Trisha},
  year={2026},
  month={jan},
  url={https://trishasalas.com/posts/testing-accessibility-knowledge-across-pythia-model-sizes/},
  note={Blog post}
}

@inproceedings{brown2020language,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom B. and Mann, Benjamin and Ryder, Nick and others},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020},
  publisher={Curran Associates, Inc.}
}

@article{meng2022locating,
  title={Locating and Editing Factual Associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17359--17372},
  year={2022}
}

@article{miletic2024semantics,
  title={Semantics of Multiword Expressions in Transformer-Based Models: A Survey},
  author={Mileti\'c, Filip and Schulte im Walde, Sabine},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={1399--1419},
  year={2024},
  publisher={MIT Press}
}

@inproceedings{haviv2023understanding,
  title={Understanding Transformer Memorization Recall Through Idioms},
  author={Haviv, Adi and Cohen, Ido and Gidron, Jacob and Schuster, Roei and Goldberg, Yoav and Geva, Mor},
  booktitle={Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},
  year={2023},
  publisher={Association for Computational Linguistics},
  address={Dubrovnik, Croatia}
}

@article{duan2025syntax,
  title={How Syntax Specialization Emerges in Language Models},
  author={Duan, Xufeng and Yao, Zhaoqian and Zhang, Yunhao and Wang, Shaonan and Cai, Zhenguang G.},
  journal={arXiv preprint arXiv:2505.19548},
  year={2025}
}

@article{power2022grokking,
  title={Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets},
  author={Power, Alethea and Burda, Yuri and Edwards, Harri and others},
  journal={arXiv preprint arXiv:2201.02177},
  year={2022}
}

@inproceedings{sennrich2016neural,
  title={Neural Machine Translation of Rare Words with Subword Units},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1715--1725},
  year={2016},
  publisher={Association for Computational Linguistics},
  address={Berlin, Germany}
}